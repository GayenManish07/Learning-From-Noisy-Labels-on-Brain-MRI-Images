/home/manishgayen/bsthesis/Learning-From-Noisy-Labels-on-Brain-MRI-Images/Co-Correcting.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(self.args.checkpoint_dir)
Use default setting
{'batch_size': 256, 'lr': 0.0001, 'lr2': 1e-05, 'momentum': 0.9, 'weight_decay': 0.001, 'backbone': 'resnet50', 'optim': 'SGD', 'scheduler': None, 'workers': 4, 'forget_rate': 0.2, 'num_gradual': 10, 'exponent': 1, 'loss_type': 'coteaching_plus', 'warmup': 0, 'linear_num': 256, 'alpha': 0.4, 'beta': 0.1, 'lambda1': 300, 'K': 10.0, 'start_epoch': 0, 'epochs': 120, 'stage1': 45, 'stage2': 84, 'noise': 0.3, 'noise_type': 'pairflip', 'dataset': 'aptos', 'image_size': 224, 'classnum': 5, 'device': 'cuda:0', 'data_device': 1, 'root': '/home/fgldlb/Documents/ISIC-Archive-Downloader/NewData', 'datanum': 15000, 'train_redux': None, 'test_redux': None, 'val_redux': None, 'full_test': False, 'random_ind_redux': False, 'curriculum': 1, 'cluster_mode': 'dual', 'dim_reduce': 100, 'mix_grad': 1, 'discard': 0, 'gamma': 0.6, 'finetune_schedule': 0, 'dir': 'experiment/test-debug', 'random_seed': None, 'checkpoint_dir': 'experiment/test-debug/checkpoint.pth.tar', 'modelbest_dir': 'experiment/test-debug/model_best.pth.tar', 'record_dir': 'experiment/test-debug/record.json', 'y_file': 'experiment/test-debug/y.npy'}
nb clases 5
Actual noise 0.30
Noise added. Actual noise rate: 0.30
Train num: 2930	Val num: 366	Test num: 366
=> loading checkpoint 'experiment/test-debug/checkpoint.pth.tar'
=> loaded checkpoint 'experiment/test-debug/checkpoint.pth.tar' (epoch 111)
=> loading record file experiment/test-debug/record.json
=> loaded record file experiment/test-debug/record.json
-----------------
Train Epoch [111/120]  Batch [0/12]  Time 17.980 (17.980)  LossA 0.047 (0.047)  LossB 0.047 (0.047)  Prec1A 53.906 (53.906)  Prec1B 53.516 (53.516)Train Epoch [111/120]  Batch [1/12]  Time 0.851 (9.415)  LossA 0.045 (0.046)  LossB 0.045 (0.046)  Prec1A 56.641 (55.273)  Prec1B 56.641 (55.078)Train Epoch [111/120]  Batch [2/12]  Time 0.859 (6.563)  LossA 0.045 (0.045)  LossB 0.045 (0.045)  Prec1A 55.469 (55.339)  Prec1B 54.688 (54.948)Train Epoch [111/120]  Batch [3/12]  Time 0.858 (5.137)  LossA 0.044 (0.045)  LossB 0.044 (0.045)  Prec1A 56.250 (55.566)  Prec1B 57.031 (55.469)Train Epoch [111/120]  Batch [4/12]  Time 12.836 (6.677)  LossA 0.045 (0.045)  LossB 0.045 (0.045)  Prec1A 55.469 (55.547)  Prec1B 55.469 (55.469)Train Epoch [111/120]  Batch [5/12]  Time 0.850 (5.706)  LossA 0.047 (0.046)  LossB 0.047 (0.046)  Prec1A 55.078 (55.469)  Prec1B 55.078 (55.404)Train Epoch [111/120]  Batch [6/12]  Time 0.853 (5.013)  LossA 0.047 (0.046)  LossB 0.047 (0.046)  Prec1A 52.734 (55.078)  Prec1B 53.906 (55.190)Train Epoch [111/120]  Batch [7/12]  Time 0.846 (4.492)  LossA 0.046 (0.046)  LossB 0.046 (0.046)  Prec1A 56.250 (55.225)  Prec1B 56.641 (55.371)Train Epoch [111/120]  Batch [8/12]  Time 13.778 (5.524)  LossA 0.054 (0.047)  LossB 0.054 (0.047)  Prec1A 50.391 (54.688)  Prec1B 50.781 (54.861)Train Epoch [111/120]  Batch [9/12]  Time 0.851 (5.056)  LossA 0.048 (0.047)  LossB 0.048 (0.047)  Prec1A 54.688 (54.688)  Prec1B 53.516 (54.727)Train Epoch [111/120]  Batch [10/12]  Time 0.868 (4.676)  LossA 0.041 (0.046)  LossB 0.041 (0.046)  Prec1A 57.422 (54.936)  Prec1B 56.250 (54.865)Train Epoch [111/120]  Batch [11/12]  Time 0.391 (4.319)  LossA 0.049 (0.047)  LossB 0.049 (0.047)  Prec1A 52.632 (54.846)  Prec1B 52.632 (54.778)
 * Top1 acc:	A: 54.846	B: 54.778
 * Label accu A: 0.9942	B: 0.9942	Pure ratio A: 0.7728	B: 0.7770
 * n2t_A: 0.3010	n2t_B: 0.3010	t2n_A: 0.0027	t2n_B: 0.0027
Val Epoch [111/120]  Batch [0/2]  Time 15.522 (15.522)  LossA 1.355 (1.355)  LossB 1.353 (1.353)  Prec1A 76.562 (76.562)  Prec1B 76.172 (76.172)Val Epoch [111/120]  Batch [1/2]  Time 0.095 (7.809)  LossA 1.191 (1.273)  LossB 1.194 (1.274)  Prec1A 74.545 (75.956)  Prec1B 73.636 (75.410)
 * Top1 acc:	A: 75.956	B: 75.410
Test Epoch [111/120]  Batch [0/2]  Time 17.996 (17.996)  LossA 1.450 (1.450)  LossB 1.440 (1.440)  Prec1A 70.312 (70.312)  Prec1B 70.312 (70.312)Test Epoch [111/120]  Batch [1/2]  Time 0.101 (9.048)  LossA 1.353 (1.401)  LossB 1.346 (1.393)  Prec1A 74.545 (71.585)  Prec1B 74.545 (71.585)
 * Top1 acc:	A: 71.585	B: 71.585
Epoch 111 using 1.0 min 25.89 sec
-----------------
Train Epoch [112/120]  Batch [0/12]  Time 15.284 (15.284)  LossA 0.047 (0.047)  LossB 0.047 (0.047)  Prec1A 54.297 (54.297)  Prec1B 54.297 (54.297)Train Epoch [112/120]  Batch [1/12]  Time 2.807 (9.046)  LossA 0.047 (0.047)  LossB 0.047 (0.047)  Prec1A 55.078 (54.688)  Prec1B 55.078 (54.688)Train Epoch [112/120]  Batch [2/12]  Time 0.978 (6.357)  LossA 0.046 (0.046)  LossB 0.046 (0.046)  Prec1A 55.859 (55.078)  Prec1B 55.469 (54.948)Train Epoch [112/120]  Batch [3/12]  Time 0.942 (5.003)  LossA 0.038 (0.044)  LossB 0.038 (0.044)  Prec1A 58.984 (56.055)  Prec1B 58.203 (55.762)Train Epoch [112/120]  Batch [4/12]  Time 12.632 (6.529)  LossA 0.049 (0.045)  LossB 0.049 (0.045)  Prec1A 53.125 (55.469)  Prec1B 52.734 (55.156)Train Epoch [112/120]  Batch [5/12]  Time 1.704 (5.725)  LossA 0.049 (0.046)  LossB 0.049 (0.046)  Prec1A 54.297 (55.273)  Prec1B 53.516 (54.883)Train Epoch [112/120]  Batch [6/12]  Time 0.900 (5.035)  LossA 0.052 (0.047)  LossB 0.052 (0.047)  Prec1A 50.000 (54.520)  Prec1B 51.562 (54.408)Train Epoch [112/120]  Batch [7/12]  Time 0.864 (4.514)  LossA 0.042 (0.046)  LossB 0.042 (0.046)  Prec1A 56.250 (54.736)  Prec1B 57.031 (54.736)Train Epoch [112/120]  Batch [8/12]  Time 13.052 (5.462)  LossA 0.051 (0.047)  LossB 0.051 (0.047)  Prec1A 53.125 (54.557)  Prec1B 52.734 (54.514)Train Epoch [112/120]  Batch [9/12]  Time 1.941 (5.110)  LossA 0.047 (0.047)  LossB 0.047 (0.047)  Prec1A 56.641 (54.766)  Prec1B 55.078 (54.570)Train Epoch [112/120]  Batch [10/12]  Time 0.880 (4.726)  LossA 0.043 (0.046)  LossB 0.043 (0.046)  Prec1A 57.031 (54.972)  Prec1B 57.422 (54.830)Train Epoch [112/120]  Batch [11/12]  Time 0.417 (4.367)  LossA 0.050 (0.047)  LossB 0.050 (0.047)  Prec1A 51.754 (54.846)  Prec1B 51.754 (54.710)
 * Top1 acc:	A: 54.846	B: 54.710
 * Label accu A: 0.9942	B: 0.9942	Pure ratio A: 0.7739	B: 0.7722
 * n2t_A: 0.3010	n2t_B: 0.3010	t2n_A: 0.0027	t2n_B: 0.0027
Val Epoch [112/120]  Batch [0/2]  Time 15.470 (15.470)  LossA 1.357 (1.357)  LossB 1.367 (1.367)  Prec1A 76.562 (76.562)  Prec1B 76.172 (76.172)Val Epoch [112/120]  Batch [1/2]  Time 0.099 (7.784)  LossA 1.192 (1.275)  LossB 1.208 (1.288)  Prec1A 74.545 (75.956)  Prec1B 73.636 (75.410)
 * Top1 acc:	A: 75.956	B: 75.410
Test Epoch [112/120]  Batch [0/2]  Time 17.882 (17.882)  LossA 1.450 (1.450)  LossB 1.450 (1.450)  Prec1A 70.312 (70.312)  Prec1B 70.312 (70.312)Test Epoch [112/120]  Batch [1/2]  Time 0.101 (8.992)  LossA 1.359 (1.405)  LossB 1.357 (1.404)  Prec1A 74.545 (71.585)  Prec1B 74.545 (71.585)
 * Top1 acc:	A: 71.585	B: 71.585
Epoch 112 using 1.0 min 26.33 sec
-----------------
Train Epoch [113/120]  Batch [0/12]  Time 16.141 (16.141)  LossA 0.054 (0.054)  LossB 0.054 (0.054)  Prec1A 51.172 (51.172)  Prec1B 50.781 (50.781)Train Epoch [113/120]  Batch [1/12]  Time 1.930 (9.035)  LossA 0.049 (0.052)  LossB 0.049 (0.052)  Prec1A 50.781 (50.977)  Prec1B 50.781 (50.781)Train Epoch [113/120]  Batch [2/12]  Time 0.870 (6.314)  LossA 0.051 (0.052)  LossB 0.051 (0.052)  Prec1A 52.734 (51.562)  Prec1B 51.953 (51.172)Train Epoch [113/120]  Batch [3/12]  Time 0.944 (4.971)  LossA 0.055 (0.052)  LossB 0.055 (0.052)  Prec1A 48.438 (50.781)  Prec1B 49.219 (50.684)Train Epoch [113/120]  Batch [4/12]  Time 12.422 (6.461)  LossA 0.039 (0.050)  LossB 0.039 (0.050)  Prec1A 59.375 (52.500)  Prec1B 59.375 (52.422)Train Epoch [113/120]  Batch [5/12]  Time 1.710 (5.670)  LossA 0.044 (0.049)  LossB 0.044 (0.049)  Prec1A 55.859 (53.060)  Prec1B 56.250 (53.060)Train Epoch [113/120]  Batch [6/12]  Time 0.911 (4.990)  LossA 0.044 (0.048)  LossB 0.044 (0.048)  Prec1A 57.031 (53.627)  Prec1B 57.031 (53.627)Train Epoch [113/120]  Batch [7/12]  Time 0.868 (4.475)  LossA 0.048 (0.048)  LossB 0.048 (0.048)  Prec1A 53.125 (53.564)  Prec1B 53.516 (53.613)Train Epoch [113/120]  Batch [8/12]  Time 13.444 (5.471)  LossA 0.047 (0.048)  LossB 0.047 (0.048)  Prec1A 53.906 (53.602)  Prec1B 53.516 (53.602)Train Epoch [113/120]  Batch [9/12]  Time 0.858 (5.010)  LossA 0.047 (0.048)  LossB 0.047 (0.048)  Prec1A 57.422 (53.984)  Prec1B 56.641 (53.906)Train Epoch [113/120]  Batch [10/12]  Time 0.855 (4.632)  LossA 0.037 (0.047)  LossB 0.037 (0.047)  Prec1A 61.328 (54.652)  Prec1B 61.328 (54.581)Train Epoch [113/120]  Batch [11/12]  Time 0.380 (4.278)  LossA 0.049 (0.047)  LossB 0.049 (0.047)  Prec1A 57.018 (54.744)  Prec1B 57.895 (54.710)
 * Top1 acc:	A: 54.744	B: 54.710
 * Label accu A: 0.9942	B: 0.9942	Pure ratio A: 0.7767	B: 0.7792
 * n2t_A: 0.3010	n2t_B: 0.3010	t2n_A: 0.0027	t2n_B: 0.0027
